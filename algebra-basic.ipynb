{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Definitions of some basic terms\n",
    "\n",
    "## Scalar\n",
    "* It's number of some kind (real, natural, etc)\n",
    "* Named usually with lower case italics and defined by specifying it's kind, example: $s \\in I\\!R$\n",
    "\n",
    "## Vector\n",
    "* It's an ordered array of scalars\n",
    "* If we considered a vector a point in space each element is a coordinate on it's corresponding axis\n",
    "* Named optionally with the vector symbol (lower-case bold or with arrow symbol): $\\mathbf{v}$ or $\\vec{v}$\n",
    "* If not specified otherwise vectors have dimension nx1 (meaning they are column-vectors)\n",
    "* Examples:\n",
    "    * $\\vec{v} \\in I\\!R^n$ \n",
    "    * $\\vec{v}= \\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n} \\end{pmatrix} $\n",
    "* Vectors have:\n",
    " * Direction\n",
    " * Magnitude (noted as $\\| \\mathbf{v} \\|$)\n",
    "\n",
    "### p-norm or $L^p$-norm\n",
    "\n",
    "Having a vector $\\vec{x} = (x_1 \\cdots x_n)$ the p-norn $\\| \\mathbf{\\vec{x}} \\|_p, p \\in I\\!R, p \\ge 1$ is defined as:\n",
    "\n",
    "* $\\| \\mathbf{\\vec{x}} \\|_p = \\sqrt[p]{|x_1|^p + \\cdots + |x_n|^p}$\n",
    "* Distances between two points:\n",
    " * With $p=1$ it's called the \"Manhattan distance\" (a.k.a. rectilinear or taxicab distance)\n",
    " * With $p=2$ is called the Euclidean distance (we use this one to calculate the length or magnitude of a vector)\n",
    "\n",
    "For Python's Numpy:\n",
    "* Euclidean norm:\n",
    " * numpy.linalg.norm(myVector) or np.linalg.norm(v1 - v2)\n",
    "* p-norm for vectos:  numpy.linalg.norm(myVector, ord=p)\n",
    "\n",
    "## Matrices\n",
    "* Two dimensional arrays of scalars\n",
    "* Examples:\n",
    "    * $A \\in I\\!R^{m \\times n}$\n",
    "    * $A = \\begin{pmatrix} a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ a_{m,1} & a_{m,2} & \\cdots & a_{m,n}  \\end{pmatrix}$\n",
    "* Two matrices can be added or subtracted if they have the same dimensions\n",
    "\n",
    "For Python's Numpy:\n",
    "* myNdArray = numpy.array(myMatrix)\n",
    "\n",
    "### Transpose\n",
    "\n",
    "For Python's Numpy:\n",
    "* myNdArray.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "A=\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nA'=\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(f'A=\\n{A}\\n\\n'\n",
    "      f'A\\'=\\n{A.T}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matrix element-wise multiplication (Hadamard product)\n",
    "\n",
    "$C = A \\circ B$\n",
    "\n",
    "* $A=\\begin{pmatrix}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{pmatrix}, B=\\begin{pmatrix}b_{11} & b_{12} \\\\ b_{21} & b_{22}\\end{pmatrix}$\n",
    "\n",
    "* $C=\\begin{pmatrix}a_{11}*b_{11} & a_{12}*b_{12} \\\\ a_{21}*b_{21} & a_{22}*b_{22}\\end{pmatrix}$\n",
    "\n",
    "\n",
    "### Matrix multiplication (dot product)\n",
    "$C=A \\cdot B$\n",
    " * Two matrices (A and B) can be multiplied if the number of columns of the first equals the number rows on the second\n",
    " * If $A \\in I\\!R^{m \\times n}$, $B \\in I\\!R^{n \\times p}$ then $C \\in I\\!R^{m \\times p}$  \n",
    " * $C = A \\cdot B : c_{i,j} = \\sum_{k} a_{i,k}b_{k,j} $ \n",
    "   * $c_{i,j}$ contains the total sum of the element-wise multiplication of (i)th row of A with (j)th col of B \n",
    "\n",
    "Examples:\n",
    "* $A=\\begin{pmatrix} a_1 \\\\ a_2 \\\\a_3 \\end{pmatrix} , X=\\begin{pmatrix} x_1 & x_2 & 1 \\end{pmatrix}, AX=(a_1x_1+a_2x_2+a_3)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "A=[1 2 3]\nB=[[4]\n [5]\n [1]])\nA*B=[17]\n =[1*4+2*5+3*1]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A=np.array([1, 2, 3])\n",
    "\n",
    "\"\"\"transpose documentation:\n",
    "    For a 1-D array this has no effect, as a transposed vector is simply the same vector.\n",
    "    To convert a 1-D array into a 2D column vector, an additional dimension must be added.\n",
    "    np.atleast_2d(a).T achieves this, as does a[:, np.newaxis].\"\"\"\n",
    "x1,x2=4,5\n",
    "B=np.array([x1, x2, 1])\n",
    "B = np.atleast_2d(B).T\n",
    "\n",
    "print(f'A={A}\\nB={B})')\n",
    "print(f'A*B={A.dot(B)}')\n",
    "print(f' =[{A[0]}*{B[0,0]}+{A[1]}*{B[1,0]}+{A[2]}*{B[2,0]}]')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* $A=\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix},\\ B=\\begin{pmatrix} b_{11} & b_{12} & b_{13}\\\\ b_{21} & b_{22} & b_{23}\\end{pmatrix}$\n",
    "\n",
    "* $C=A \\cdot B = \\begin{pmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}*b_{12} + a_{12}*b_{22} & a_{11}*b_{13} + a_{12}*b_{23}\\\\ a_{21}b_{11} + a_{22}b_{21} & a_{21}*b_{12} + a_{22}*b_{22} & a_{21}*b_{13} + a_{22}*b_{23}\\end{pmatrix}$ "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "A=\n[[1 2]\n [3 4]]\nB=\n[[ 5  6  7]\n [ 8  9 10]]\nAB=\n[[21 24 27]\n [47 54 61]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A=np.array([[1, 2], [3, 4]])\n",
    "B=np.array([[5, 6, 7], [8, 9, 10]])\n",
    "print(f'A=\\n{A}')\n",
    "print(f'B=\\n{B}')\n",
    "print(f'AB=\\n{A.dot(B)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Matrix multiplication properties:\n",
    "* $A \\cdot B \\neq B \\cdot A$\n",
    "* $A \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C$\n",
    "* $A \\cdot (B+C) = A \\cdot B + A \\cdot C$ \n",
    "* $(A+B) \\cdot C = A \\cdot C + B \\cdot C$\n",
    "\n",
    "For Python's Numpy:\n",
    "* element-wise product: myNdArray1 * myNdArray2\n",
    "* dot product: myNdArray1.dot(myNdArray2)\n",
    "\n",
    "### Dot multiplication of two vectors\n",
    "\n",
    "* $\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|cos(\\theta)$\n",
    "* We can simplify to $\\vec{a} \\cdot \\vec{b} = \\|\\vec{b}\\|\\ scalarProj_ba$\n",
    " * $cos(\\theta)$ is equal to the length of the projection of $\\vec{a}$ over $\\vec{b}$ (or the scalar projection) divided by $\\|\\vec{a}\\|$\n",
    "   * The scalar projection of $\\vec{a}$ over $\\vec{b}$ is\n",
    "     * $a_b = \\|\\vec{a}\\|cos(\\theta)$\n",
    "   * $proj_ba = a_b\\ \\hat{b}$ where $\\hat{b}$ is the unit vector in the direction of $\\vec{b}$\n",
    "\n",
    "We can see that the dot product of two vectors, in a way, meassures \"how much the go in the same direction\" so to speak.\n",
    "\n",
    "Properties:\n",
    "* Two non-zero vectors are orthogonal if and only if $\\vec{a} \\cdot \\vec{b} = 0$\n",
    "* $\\vec{a} \\cdot \\vec{a} = \\|\\vec{a}\\|$\n",
    "\n",
    "### Some alternative ways to understand what the dot product of a matrix and a vector does...\n",
    "\n",
    "* A transformation:\n",
    " * Having $A \\in I\\!R^{m \\times n}, \\vec{x} \\in I\\!R^{n \\times 1}$\n",
    " * Then we can see the dot product result as an \"output-space\" ($I\\!R^{m \\times 1}$) we want to \"translate\" or \"transform\" $\\vec{x}$ to.\n",
    "* Linear or weighted combination of column-vectors:\n",
    " * Having $A=\\begin{pmatrix} \\vec{v_1} & \\cdots & \\vec{v_n}\\end{pmatrix},\\ \\vec{x}=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}$\n",
    " * Then $A \\cdot \\vec{x} = x_1\\vec{v_1} + \\cdots + x_n\\vec{v_n}$\n",
    "* Dot product of row-vectors\n",
    " * Having $A=\\begin{pmatrix} \\vec{v_1}^T \\\\ \\vdots \\\\ \\vec{v_m}^T\\end{pmatrix},\\ \\vec{x}=\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}$\n",
    " * Then $A \\cdot \\vec{x} =\\begin{pmatrix} \\vec{v_1}^T \\cdot \\vec{x}\\\\ \\vdots \\\\ \\vec{v_m}^T\\cdot \\vec{x}\\end{pmatrix}$\n",
    "\n",
    "\n",
    "### Identity matrix and inverse\n",
    "\n",
    "The identity matrix $I_n$ is a $n \\times n$ matrix contains 1s on the upper-left to lower-right diagonal and zeros\n",
    "on the rest. E.g.:\n",
    "\n",
    "$I_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n",
    "\n",
    "Identity matrix properties (acts like one for real numbers on multiplication):\n",
    "$A \\cdot I = I \\cdot A = A$\n",
    "\n",
    "The inverse matrix $A^{-1}$ verifies that $A\\cdot A^{-1} = I$.\n",
    "Not all matrices have inverse (for those cases some math libraries include functions to obtain pseudo-inverse matrices).\n",
    "\n",
    "A singular matrix is a square matrix that does not have an inverse.\n",
    "\n",
    "For Python's Numpy:\n",
    "* numpy.identity(n)\n",
    "* numpy.linalg.inv(myMatrix)\n",
    "* numpy.linalg.pinv(myMatrix)\n",
    "\n",
    "\n",
    "Examples:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "A=\n[[1 2]\n [3 4]]\n\ninvA=\n[[-1.9999999999999996  0.9999999999999998]\n [ 1.4999999999999998 -0.4999999999999999]]\n\npinvA=\n[[-2.0000000000000018  1.0000000000000007]\n [ 1.5000000000000018 -0.5000000000000007]]\n\nA * invA=\n[[1.000000000000000e+00 0.000000000000000e+00]\n [8.881784197001252e-16 9.999999999999996e-01]]\n\nA * pinvA=\n[[ 1.0000000000000018e+00 -6.6613381477509392e-16]\n [ 1.7763568394002505e-15  9.9999999999999911e-01]]\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,2],[3,4]])\n",
    "invA = np.linalg.inv(A)\n",
    "pinvA = np.linalg.pinv(A) #Moore-Penrose psuedo inverse\n",
    "\n",
    "np.set_printoptions(precision=20, suppress=False)\n",
    "print(f'A=\\n{A}\\n')\n",
    "print(f'invA=\\n{invA}\\n')\n",
    "print(f'pinvA=\\n{pinvA}\\n')\n",
    "print(f'A * invA=\\n{A.dot(invA)}\\n')\n",
    "print(f'A * pinvA=\\n{A.dot(pinvA)}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eigenvectors and eigenvalues \n",
    "\n",
    "Eigenvectors are also called characteristic vectors.\n",
    "\n",
    "Having:\n",
    "* $A \\cdot X = \\lambda X : A \\in C^{n \\times n}, X \\in C^n, X\\ is\\ a\\ non-zero\\ vector$\n",
    "* we call X an eigenvector of A and the corresponding $\\lambda$ an eigenvalue.\n",
    "* X changes length but not direction\n",
    "* Also true $A \\cdot X - \\lambda X = 0$ and $(A - \\lambda I)\\cdot X = 0$ y $(\\lambda I - A)\\cdot X=0: X\\neq0$\n",
    "\n",
    "The idea is we are able to identify with vectors don't change direction.\n",
    "\n",
    "#### In the context of graph theory\n",
    "With $A$ being the adjacency matrix (can be weighted) of graph $G$:\n",
    "* $A$'s greatest eigenvalue and it's corresponding eigenvector are used to study nodes centrality\n",
    "* Eigenvector centrality (here $c$) of a node gives us an indication of it's relative influence or relevance on the graph\n",
    " * If a node is pointed to by many nodes with high centrality score said node will gave a high score\n",
    " * $c(x_i) = \\frac{1}{\\lambda} \\sum_{x_j \\in N(x_i)} c(x_j) = \\frac{1}{\\lambda} \\sum_{x_j \\in G} [connected(x_i, x_j)] c(x_j)$\n",
    " (NOTE! \"[\" and \"]\" here are Iverson's brackets, 1 if condition is true and 0 otherwise).\n",
    "* This idea is used by Google's pagerank algorithm\n",
    "\n",
    "#### Eigenvectors are also used with the covariance matrix:\n",
    "* Covariance measures how much two much two random variables vary together\n",
    "* Covariance matrix $C \\in R^{d \\times d}, C_{i,j}=\\sigma(x_i, x_j)$\n",
    " * d is the number of dimensions (variables, features, etc)\n",
    " * $x_i, x_j$ are random variables\n",
    " * $\\sigma(x_i, x_j)= \\sigma(x_j, x_i)$\n",
    "* In this context \"The eigenvectors are unit vectors representing the direction of the largest variance of the data,\n",
    "while the eigenvalues represent the magnitude of this variance in the corresponding directions.\" Source:https://datascienceplus.com/understanding-the-covariance-matrix/\n",
    " * This is used on Principal Component Analysis (PCA)\n",
    "\n",
    "For Python's Numpy:\n",
    "* w,v = numpy.linalg.eig(myMatrix) -> returns eigenvalues & normalized eigenvectors\n",
    " * column v[:,i] is the eigenvector corresponding to the eigenvalue w[i]\n",
    " * TIP: if we need eigenvectors as rows use v.T instead\n",
    "\n",
    "Example 1:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue=-0.3722813232690143,\n",
      "eigenvector=[-0.82456484  0.56576746]\n",
      "=> A * eigenVector=[ 0.30697009 -0.21062466]\n",
      "=> eigenvalue * eigenvector=[ 0.30697009 -0.21062466]\n",
      "\n",
      "eigenvalue=5.372281323269014,\n",
      "eigenvector=[-0.41597356 -0.90937671]\n",
      "=> A * eigenVector=[-2.23472698 -4.88542751]\n",
      "=> eigenvalue * eigenvector=[-2.23472698 -4.88542751]\n",
      "\n",
      "Calculating all at once.\n",
      "A * eigenVectorMatrix=\n",
      "[[ 0.30697009 -2.23472698]\n",
      " [-0.21062466 -4.88542751]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,2],[3,4]])\n",
    "e_values, e_vectors = np.linalg.eig(A)\n",
    "e_vectors_aux = e_vectors.T\n",
    "\n",
    "for l, v in zip(e_values, e_vectors_aux):\n",
    "    print(f'eigenvalue={l},\\neigenvector={v}')\n",
    "    print(f'=> A * eigenVector={A.dot(v.T)}') #works even without \".T\" but beware of dimensions!!!\n",
    "    print(f'=> eigenvalue * eigenvector={l*v}\\n')\n",
    "\n",
    "print(f'Calculating all at once.')\n",
    "print(f'A * eigenVectorMatrix=\\n{A.dot(e_vectors)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example 2, simple tree graph. Check root node has higher eigenvector centrality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue=0.7615773105863907,\n",
      "eigenvector=[0.70710678 0.27854301 0.64993368]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Graph connectivity matrix\n",
    "A = np.array([[0,.3,.7],[.3, 0, 0], [.7, 0, 0]])\n",
    "e_values, e_vectors = np.linalg.eig(A)\n",
    "e_vectors_aux = e_vectors.T\n",
    "\n",
    "for l, v in zip(e_values, e_vectors_aux):\n",
    "    if l>0:\n",
    "        print(f'eigenvalue={l},\\neigenvector={v}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Putting it all together\n",
    "\n",
    "### Matrix and equation systems\n",
    "\n",
    "Let' assume we have the following system of equations:\n",
    "* $3x_1 + 2x_2 = 13$\n",
    "* $6x_1 - 3x_2 = 6$\n",
    "\n",
    "We can rewrite then as:\n",
    "\n",
    "$ A = \\begin{pmatrix} 3 & 2 \\\\ 6 & -3 \\end{pmatrix},\\ \\vec{x}= \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix},\\ \\vec{b}=\\begin{pmatrix}13 \\\\ 6\\end{pmatrix}$\n",
    "\n",
    "$A \\cdot \\vec{x} = \\vec{b}$\n",
    "\n",
    "Let's solve using what we've learned so far:\n",
    "\n",
    "* $A^{-1} \\cdot A \\cdot \\vec{x} =  A^{-1} \\cdot \\vec{b}$\n",
    "* $I \\cdot \\vec{x} =  A^{-1} \\cdot \\vec{b}$\n",
    "* $\\vec{x} =  A^{-1} \\cdot \\vec{b}$\n",
    "\n",
    "Once we've calculated $A^{-1}$ we can even use it to solve with different values of $\\vec{b}$.\n",
    "\n",
    "\n",
    "\n",
    "#### Example 1 with Python's Numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution for b1, x=\n",
      "[[2.42857143]\n",
      " [2.85714286]]\n",
      "Check A.dot(x)=\n",
      "[[13.]\n",
      " [ 6.]]\n",
      "\n",
      "Solution for b2, x=\n",
      "[[3.28571429]\n",
      " [5.57142857]]\n",
      "Check A.dot(x)=\n",
      "[[21.]\n",
      " [ 3.]]\n",
      "\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-9cee6b9926a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mA\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#What happends here?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0minvA\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mA\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#Will throw \"LinAlgError: Singular matrix\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;31m#invA = np.linalg.pinv(A) #Will give as a WRONG answer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Solution for b1, x={invA.dot(b1)}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36minv\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/python/AI-bootcamp/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001B[0m in \u001B[0;36minv\u001B[0;34m(a)\u001B[0m\n\u001B[1;32m    544\u001B[0m     \u001B[0msignature\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'D->D'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misComplexType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'd->d'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    545\u001B[0m     \u001B[0mextobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_linalg_error_extobj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_raise_linalgerror_singular\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 546\u001B[0;31m     \u001B[0mainv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_umath_linalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextobj\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mextobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    547\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mainv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult_t\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    548\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/AI-bootcamp/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001B[0m in \u001B[0;36m_raise_linalgerror_singular\u001B[0;34m(err, flag)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_raise_linalgerror_singular\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mLinAlgError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Singular matrix\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_raise_linalgerror_nonposdef\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLinAlgError\u001B[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[3,2],[6,-3]])\n",
    "b1 = np.array([[13],[6]])\n",
    "b2 = np.array([[21],[3]])\n",
    "\n",
    "invA = np.linalg.inv(A)\n",
    "\n",
    "print(f'Solution for b1, x=\\n{invA.dot(b1)}')\n",
    "print(f'Check A.dot(x)=\\n{A.dot(invA.dot(b1))}\\n')\n",
    "print(f'Solution for b2, x=\\n{invA.dot(b2)}')\n",
    "print(f'Check A.dot(x)=\\n{A.dot(invA.dot(b2))}\\n')\n",
    "\n",
    "A = np.array([[3,0],[6,0]]) #What happends here?\n",
    "invA = np.linalg.inv(A) #Will throw \"LinAlgError: Singular matrix\"\n",
    "#invA = np.linalg.pinv(A) #Will give as a WRONG answer\n",
    "print(f'Solution for b1, x={invA.dot(b1)}')\n",
    "print(f'Check our result A.dot(x)={A.dot(invA.dot(b1))}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solve for the coefficients of a quadratic function\n",
    "\n",
    "$ax^2+bx+c=y$\n",
    "* we need to known three points (one for each unknown)\n",
    "* let's say we know $(x_1, y_1),\\ (x_2, y_2),\\ (x_3, y_3)$\n",
    "\n",
    "We can write:\n",
    "\n",
    "$\\begin{pmatrix}x_1^2 & x_1 & 1 \\\\ x_2^2 & x_2 & 1 \\\\ x_3^2 & x_3 & 1\\end{pmatrix} \\cdot \\begin{pmatrix} a \\\\ b \\\\ c\\end{pmatrix} =  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3\\end{pmatrix}$\n",
    "\n",
    "It's the same as before $A \\cdot \\vec{x} = \\vec{b}$\n",
    "\n",
    "Again $\\vec{x} =  A^{-1} \\cdot \\vec{b}$\n",
    "\n",
    "\n",
    "#### Example 2 with Python's Numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution invA.dot(b) = [[-4.4408921e-16]\n",
      " [ 1.0000000e+00]\n",
      " [ 3.0000000e+00]]\n",
      "\n",
      "Equation:\n",
      "-4.440892098500626e-16 * x^2 + 1.0 * x + 3.0000000000000018 = y\n",
      "\n",
      "Check our result A.dot(x)=\n",
      "[[4.]\n",
      " [5.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,1,1],[4,2,1], [9,3,1]])\n",
    "b = np.array([[4],[5],[6]])\n",
    "invA = np.linalg.inv(A)\n",
    "x = invA.dot(b)\n",
    "print(f'Solution invA.dot(b) = {x}\\n')\n",
    "print(f'Equation:\\n{x[0][0]} * x^2 + {x[1][0]} * x + {x[2][0]} = y\\n')\n",
    "\n",
    "print(f'Check our result A.dot(x)=\\n{A.dot(x)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors\n",
    "* multidimensional arrays (matrices are 2D tensors)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}